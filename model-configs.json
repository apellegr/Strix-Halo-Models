{
  "version": "1.0",
  "description": "Optimized model configurations for Strix Halo (AMD Ryzen AI Max+ 395)",
  "defaults": {
    "threads": 16,
    "batch_size": 512,
    "ctx_size": 4096
  },
  "models": {
    "qwen3-235b-thinking": {
      "gpu_layers": 50,
      "ctx_size": 4096,
      "batch_size": 512,
      "benchmark": {
        "pp_tokens_per_sec": 129.25,
        "tg_tokens_per_sec": 8.33,
        "memory_gb": 51,
        "date": "2026-01-06"
      },
      "notes": "Optimal for 128GB unified memory. Higher GPU layers cause OOM."
    },
    "deepseek-coder-v2-16b": {
      "gpu_layers": 80,
      "ctx_size": 4096,
      "batch_size": 512,
      "benchmark": {
        "pp_tokens_per_sec": 1164.224139,
        "tg_tokens_per_sec": 63.227008,
        "memory_gb": 17,
        "date": "2026-01-07"
      }
    },
    "llama-3.2-3b": {
      "gpu_layers": 50,
      "ctx_size": 8192,
      "batch_size": 1024,
      "benchmark": {
        "pp_tokens_per_sec": 2154.124430,
        "tg_tokens_per_sec": 68.762771,
        "memory_gb": 8,
        "date": "2026-01-07"
      }
    },
    "qwen2.5-coder-32b": {
      "gpu_layers": 80,
      "ctx_size": 32768,
      "batch_size": 1024,
      "benchmark": {
        "pp_tokens_per_sec": 316.825962,
        "tg_tokens_per_sec": 10.523251,
        "memory_gb": 24,
        "date": "2026-01-07"
      }
    },
    "deepseek-r1-32b": {
      "gpu_layers": 80,
      "ctx_size": 32768,
      "batch_size": 1024,
      "benchmark": {
        "pp_tokens_per_sec": 315.620753,
        "tg_tokens_per_sec": 10.530794,
        "memory_gb": 24,
        "date": "2026-01-07"
      }
    },
    "qwen3-coder-30b": {
      "gpu_layers": 80,
      "ctx_size": 32768,
      "batch_size": 1024,
      "notes": "MoE model with 30B params, 3B active per token. Should be much faster than dense 32B models."
    },
    "hermes-4-70b": {
      "gpu_layers": 40,
      "ctx_size": 32768,
      "batch_size": 512,
      "notes": "NousResearch Hermes 4 70B - optimized for tool/function calling and structured outputs. ~42GB model. Reduced GPU layers to avoid VRAM OOM when running with other models."
    },
    "hermes-4-14b": {
      "gpu_layers": 80,
      "ctx_size": 32768,
      "batch_size": 1024,
      "notes": "NousResearch Hermes 4 14B - fast model optimized for tool/function calling. ~10GB model."
    }
  }
}
