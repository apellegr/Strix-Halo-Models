{
  "version": "1.0",
  "description": "Optimized model configurations for Strix Halo (AMD Ryzen AI Max+ 395)",
  "defaults": {
    "threads": 16,
    "batch_size": 512,
    "ctx_size": 4096
  },
  "models": {
    "qwen3-235b-thinking": {
      "gpu_layers": 81,
      "ctx_size": 4096,
      "batch_size": 1024,
      "benchmark": {
        "pp_tokens_per_sec_short": 97.4,
        "pp_tokens_per_sec_long": 546.2,
        "tg_tokens_per_sec": 9.0,
        "memory_gb": 90,
        "date": "2026-01-15"
      },
      "notes": "81 GPU layers (85% offload) with --no-mmap. Prompt: 97-546 tok/s (scales with length), Gen: ~9 tok/s. 2x+ faster prompt than 55 layers. Max is 81 layers (82+ OOM)."
    },
    "deepseek-coder-v2-16b": {
      "gpu_layers": 80,
      "ctx_size": 4096,
      "batch_size": 512,
      "benchmark": {
        "pp_tokens_per_sec": 1164.224139,
        "tg_tokens_per_sec": 63.227008,
        "memory_gb": 17,
        "date": "2026-01-07"
      }
    },
    "llama-3.2-3b": {
      "gpu_layers": 999,
      "ctx_size": 32768,
      "batch_size": 2048,
      "benchmark": {
        "pp_tokens_per_sec": 2154.124430,
        "tg_tokens_per_sec": 68.762771,
        "memory_gb": 8,
        "date": "2026-01-07"
      },
      "notes": "Fully GPU offloaded. Small 3B model."
    },
    "qwen2.5-coder-32b": {
      "gpu_layers": 80,
      "ctx_size": 32768,
      "batch_size": 1024,
      "benchmark": {
        "pp_tokens_per_sec": 316.825962,
        "tg_tokens_per_sec": 10.523251,
        "memory_gb": 24,
        "date": "2026-01-07"
      }
    },
    "deepseek-r1-32b": {
      "gpu_layers": 80,
      "ctx_size": 32768,
      "batch_size": 1024,
      "benchmark": {
        "pp_tokens_per_sec": 315.620753,
        "tg_tokens_per_sec": 10.530794,
        "memory_gb": 24,
        "date": "2026-01-07"
      }
    },
    "qwen3-coder-30b": {
      "gpu_layers": 80,
      "ctx_size": 32768,
      "batch_size": 1024,
      "notes": "MoE model with 30B params, 3B active per token. Should be much faster than dense 32B models."
    },
    "hermes-4-70b": {
      "gpu_layers": 999,
      "ctx_size": 32768,
      "batch_size": 2048,
      "notes": "NousResearch Hermes 4 70B - fully GPU offloaded with --no-mmap. ~40GB model uses ~45GB GPU memory. Requires TTM kernel params."
    },
    "hermes-4-14b": {
      "gpu_layers": 80,
      "ctx_size": 32768,
      "batch_size": 1024,
      "notes": "NousResearch Hermes 4 14B - fast model optimized for tool/function calling. ~10GB model."
    }
  }
}
